{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from models import *\n",
    "from utils import *\n",
    "import os, sys, time, datetime, random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='config/yolov3.cfg'\n",
    "weights_path='config/yolov3.weights'\n",
    "class_path='config/coco.names'\n",
    "img_size=416\n",
    "conf_thres=0.5\n",
    "nms_thres=0.4\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load model and put into eval mode\n",
    "model = Darknet(config_path, img_size=img_size)\n",
    "model.load_weights(weights_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "data_transforms = transforms.Compose([ transforms.ToTensor()])\n",
    "if device == torch.device(\"cpu\"):\n",
    "    Tensor = torch.FloatTensor\n",
    "else:\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    \n",
    "    \n",
    "    \n",
    "f_rnn = torch.load('f_rnn128_20e.pt')                                           #Forward  LSTM\n",
    "b_rnn = torch.load('b_best.pt')                                                 #Backward LSTM\n",
    "cnn_model = torch.load('ensembledModel.jfc_6,200_75e.pt',map_location = 'cpu')  #Ensambled CNN\n",
    "if train_on_gpu:\n",
    "    f_rnn.cuda()\n",
    "    b_rnn.cuda()\n",
    "    cnn_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames_tensor = torch.FloatTensor(1,3,256,256).to(device)\n",
    "cropped_frames_tensor = torch.FloatTensor(1,3,256,256).to(device)\n",
    "\n",
    "sequence_length = 100\n",
    "batch_size = 256\n",
    "\n",
    "video = ' '    \n",
    "cap = cv2.VideoCapture(video)\n",
    "mot_tracker = Sort()\n",
    "frames = 0\n",
    "box_h = 556\n",
    "box_w = 556\n",
    "y1 = 300\n",
    "x1 = 300\n",
    "idx=0\n",
    "n = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image(img):\n",
    "    \n",
    "    # scale and pad image\n",
    "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
    "    imw = round(img.size[0] * ratio)\n",
    "    imh = round(img.size[1] * ratio)\n",
    "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
    "\n",
    "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
    "                        (128,128,128)),\n",
    "         transforms.ToTensor(),\n",
    "         ])\n",
    "    \n",
    "    \n",
    "    # convert image to Tensor\n",
    "    image_tensor = img_transforms(img).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input_img = Variable(image_tensor.type(Tensor))\n",
    "    \n",
    "    \n",
    "    # run inference on the model and get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_img)\n",
    "        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "    return detections[0]\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "def chunk(features,seq_length):\n",
    "    for i in range(0, len(features)-seq_length, 1):\n",
    "        yield features[i:i+seq_length]\n",
    "             \n",
    "########################################################################################################################\n",
    "def batching_data(cnn_out, sequence_length, batch_size, shuffling):\n",
    "    \n",
    "    rNpArr = np.flip(cnn_out.cpu().numpy(),0).copy()   \n",
    "    reversed_cnn_out = torch.from_numpy(rNpArr)\n",
    "    \n",
    "    feature_tensors = torch.stack(list(chunk(cnn_out, sequence_length)))\n",
    "    reversed_feature_tensors = torch.stack(list(chunk(reversed_cnn_out, sequence_length)))\n",
    "    \n",
    "    data = TensorDataset(feature_tensors)\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffling,drop_last=False) \n",
    "    reversed_data = TensorDataset(reversed_feature_tensors)\n",
    "    reversed_data_loader = torch.utils.data.DataLoader(reversed_data, batch_size=batch_size,\n",
    "                                                       shuffle=shuffling,drop_last=False)\n",
    "    return data_loader, reversed_data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferOnVideo(path_to_video):\n",
    "    #frames_list = []\n",
    "    #cropped_frames_list = []\n",
    "    \n",
    "    #Reserving a space for frames and cropped frames tensors in GPU\n",
    "    frames_tensor = torch.FloatTensor(1,3,256,256).to(device)\n",
    "    cropped_frames_tensor = torch.FloatTensor(1,3,256,256).to(device)\n",
    "    \n",
    "    video = ' '\n",
    "    cap = cv2.VideoCapture(path_to_video)\n",
    "    mot_tracker = Sort()\n",
    "    frames = 0\n",
    "    box_h = 556\n",
    "    box_w = 556\n",
    "    y1 = 300\n",
    "    x1 = 300\n",
    "    idx=0\n",
    "    n = 0\n",
    "    while(True):\n",
    "        idx+=1\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "            \n",
    "        frame = cv2.cvtColor( frame, cv2.COLOR_BGR2RGB)\n",
    "        pilimg = Image.fromarray(frame)        \n",
    "        detections = detect_image(pilimg)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        img = np.array(pilimg)\n",
    "        \n",
    "        pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "        pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "        unpad_h = img_size - pad_y\n",
    "        unpad_w = img_size - pad_x\n",
    "\n",
    "        if detections is not None:\n",
    "            tracked_objects = mot_tracker.update(detections.cpu())\n",
    "            n_cls_preds = len(unique_labels)\n",
    "\n",
    "            for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
    "                box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
    "                box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
    "                y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
    "                x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
    "                \n",
    "            starty = y1-20 if y1>20 else 0    \n",
    "            startx = x1-20 if x1>20 else 0\n",
    "            \n",
    "            roi = frame[starty:y1 + box_h+50, startx:x1 + box_w+50,:]            \n",
    "        else:\n",
    "            starty = y1-20 if y1>20 else 0    \n",
    "            startx = x1-20 if x1>20 else 0\n",
    "            roi = frame[starty:y1 + box_h+50, startx:x1 + box_w+50,:]\n",
    "        if idx%2==0:\n",
    "            n+=1\n",
    "            frame = data_transforms(cv2.resize(frame, (256, 256))).to(device)\n",
    "            roi = data_transforms(cv2.resize(roi, (256, 256))).to(device)\n",
    "            frames_tensor = torch.cat((frames_tensor,frame.unsqueeze(0)),0)\n",
    "            cropped_frames_tensor = torch.cat((cropped_frames_tensor,roi.unsqueeze(0)),0)\n",
    "            print(n)\n",
    "            \n",
    "    ############################################################################################################################\n",
    "    frames_tensor = frames_tensor[1:]\n",
    "    cropped_frames_tensor = cropped_frames_tensor[1:]\n",
    "    \n",
    "    data = TensorDataset(frames_tensor, cropped_frames_tensor)\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False,drop_last=False)\n",
    "    \n",
    "    ######################################## Extracting Image Features and Predictions using resnet  ################################################\n",
    "    cnn_out200 = torch.FloatTensor(1,200).cuda()\n",
    "    cnn_out6 = torch.FloatTensor(1,6).cuda()\n",
    "    cnn_model.eval()\n",
    "    k=0\n",
    "    with torch.no_grad():\n",
    "        for data,c_data in data_loader:\n",
    "            k+=1\n",
    "            print(k)\n",
    "\n",
    "            if train_on_gpu:\n",
    "                data  , c_data  = data.cuda(), c_data.cuda()\n",
    "\n",
    "            out6,out200 = cnn_model.forward(data,c_data)\n",
    "            cnn_out6 = torch.cat((cnn_out6,out6),0)\n",
    "            cnn_out200 = torch.cat((cnn_out200,out200),0)\n",
    "    cnn_out6    = cnn_out6[1:]\n",
    "    cnn_out200  = cnn_out200[1:]\n",
    "    del data  , c_data\n",
    "    ###########################################################################################################################\n",
    "    \n",
    "    rnn_batch_size = len(cnn_out200)\n",
    "    f_data_loader200, b_data_loader200 = batching_data(cnn_out200,sequence_length,batch_size = rnn_batch_size,shuffling = False)\n",
    "    \n",
    "    ############################################### Inference #################################################################\n",
    "    \n",
    "    f_rnn.eval()\n",
    "    b_rnn.eval()\n",
    "    final_out = torch.zeros([len(cnn_out200),6 ], dtype=torch.float64).cuda()\n",
    "    fout=torch.FloatTensor(1,6).cuda()\n",
    "    bout=torch.FloatTensor(1,6).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hiddenf = f_rnn.init_hidden(len(f_data_loader200.dataset))\n",
    "        hiddenb = b_rnn.init_hidden(len(f_data_loader200.dataset))\n",
    "        for (fdata,rdata) in zip(f_data_loader200, b_data_loader200):\n",
    "\n",
    "            if train_on_gpu:\n",
    "                fdata, rdata = fdata[0].cuda(), rdata[0].cuda()\n",
    "\n",
    "            outputf, hiddenf = f_rnn(fdata, hiddenf)\n",
    "            outputb, hiddenb = b_rnn(rdata, hiddenb)\n",
    "\n",
    "            outputf = F.log_softmax(outputf, dim=1)\n",
    "            outputb = F.log_softmax(outputb, dim=1)\n",
    "\n",
    "            fout = torch.cat((fout,outputf),0)\n",
    "            bout = torch.cat((bout,outputb),0)\n",
    "\n",
    "\n",
    "    fout = fout[1:]\n",
    "    bout = bout[1:]   \n",
    "    final_out[sequence_length:] = fout\n",
    "    rNpArr = np.flip(bout.cpu().numpy(),0).copy()   \n",
    "    bout = torch.from_numpy(rNpArr)\n",
    "    final_out[:-sequence_length] += bout.double().cuda()\n",
    "    #cnn_tout6 = F.log_softmax(cnn_tout6, dim=1)\n",
    "    #final_out += cnn_tout6.double()\n",
    "    final_out /= 2\n",
    "    _, pred = torch.max(final_out, 1)\n",
    "    #acc=torch.stack(list(pred==all_ttargets.long()))\n",
    "    #print('accuracy  : ',(torch.sum(acc).item()/len(acc))*100, '%')\n",
    "    \n",
    "    \n",
    "    ###########################################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ActionRecognitionAPI import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4063\n"
     ]
    }
   ],
   "source": [
    "out = inferOnVideo('/home/eng_mohammed_saad_18/dataset/test/27_1_crop.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4063"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4., 4., 2., 2., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
       "       4., 4., 4., 4., 4., 4., 4., 4., 4., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[400:450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
